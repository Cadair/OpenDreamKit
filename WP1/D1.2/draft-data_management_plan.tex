\documentclass[12pt]{article}

\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{ae,aecompl,aeguill}	% pour utiliser << et >>
\usepackage{times}
\usepackage[babel=true,kerning=true]{microtype}
\usepackage{xspace}
\usepackage[show]{ed}
\usepackage{amsfonts}

\newcommand{\software}[1]{\textsc{#1}\xspace}
\newcommand{\Sage}{\software{Sagemath}}
\newcommand{\GAP}{\software{GAP}}
\newcommand{\PARIGP}{\software{PARI/GP}}
\newcommand{\Magma}{\software{Magma}}
\newcommand{\Q}{\(\mathbb{Q}\)}

\title{Draft Data Management Plan for OpenDreamKit}
\author{Beno√Æt Pilorget (Editor)}

\begin{document}

\maketitle
\tableofcontents\newpage

\section{Introduction}

This document aims at gathering the information about the data created in the research process within the OpenDreamKit frame:

\begin{enumerate}
\item[1] Data needed to validate the results presented in scientific publications
\item[2] Other data (i.e. curated data not directly attributable to a publication, or raw data)
\end{enumerate}
Its goal is only to collect general information on the data you can produce. 
What you must ask yourself is therefore: "what do I create in order to improve my research work?"
Any answer to that question is potential data. Data can be divided into datasets according to their nature (code, surveys, measurements etc).

Each site must fill in this document with the information they can give at the moment (a small paragraph is enough for now, but if you can do more, go on) More precise information will be needed as the project goes on.

This is only the draft of the 1st version of the Data Management Plan.
The final version of the document will be at the end of the OpenDreamKit project itself.



\section{Datasets}

\subsection{UPSud}

Most of the data created by UPsud is related to the software \Sage and will be incorporated into the \Sage codebase. There might also be smaller data sets of tutorials, documentation and teaching content independent of the \Sage codebase which will be stored accordingly to the size and needs.
\begin{description}
\item[Data storage and security] All addition to the the \Sage codebase will be stored within the distributed \Sage repository on the trac server \href{http://trac.sagemath.org/}{trac.sagemath.org}. For smaller datasets, we might use other distributed git repositories and store a central clone on platforms such as github. All the present data is public, and there is no concern about
unauthorised access. Through cloud hosting and local clones of
repositories, there are backups and redundancy.
\label{Sagesec}
\item[Dissemination] The \Sage codebase is publicly accessible through the trac server \href{http://trac.sagemath.org/}{trac.sagemath.org} and distributed within the \Sage software. For other data, we have an open access and open source policy and will advertise the data sets accordingly.
\label{Sagediss}
\item[Preservation and future access] By using the distributed system git to manage most of our data, we assure a local copy of the data within each participant machine. We rely on external platforms (trac and github) for public access. If it should happen that those platforms are not available any more, the data can easily be moved away to another platform.
\label{Sagepres}
\end{description}


\begin{enumerate}



\item {Dataset 1}


\begin{description}
\item[Name of data] Addition to the \Sage codebase
\item[Nature of data] Software code
\item[Licence] GPL
\item[Reuse of existing data] The data is added to the already large existing \Sage codebase.
\item[Mean of production] Code implementation by UPsud participants.
\item[Data standard] The code is mostly written in Python, also using the Rest syntax for documentation and \Sage coding conventions.
\item [Usage for further experiments] The code is merged in the software and can be distributed and reused through the Software. Through the git history,
one can trace back older versions of the code and re-enable a former state of the software.
\item [Link] \href{http://trac.sagemath.org/}{trac.sagemath.org}
\end{description}


\item{Dataset 2}


\begin{description}
\item[Name of data] The OpenDreamKit website
\item[Nature of data] Text and metadata concerning OpenDreamKit participants and activities
\item[Reuse of existing data]
\item[Mean of production] Written by OpenDreamKit participants
\item[Data standard] Source code is written in Markdown language and converted into html.
\item [Usage for further experiments]
\item [Link] \href{http://opendreamkit.org/}{opendreamkit.org}, \href{https://github.com/OpenDreamKit/OpenDreamKit.github.io}{https://github.com/OpenDreamKit/OpenDreamKit.github.io}
\end{description}

\end{enumerate}
\subsection{UStan}

Most of the data created by UStan is related to the software \GAP and
will be incorporated into the \GAP codebase. There might also be
smaller data sets of tutorials, documentation and teaching content
independent of the \GAP codebase which will be stored accordingly to
the size and needs.
\begin{description}
\item[Data storage and security] All addition to the the \GAP
  codebase will be stored within the distributed \GAP repository on
  github or a related repository. All the present data is
  public, and there is no concern about unauthorised access. Through
  cloud hosting and local clones of repositories, there are backups
  and redundancy.\label{GAPsec}
\item[Dissemination] The \GAP codebase is publicly accessible through
  github and mostly distributed as part of \GAP.
  For other data, we have an open access and open source policy and will advertise the data sets accordingly.
\label{GAPdiss}

\item[Preservation and future access] By using the distributed system
  git to manage most of our data, we assure a local copy of the data
  within each participant machine. We rely on external platform for public access. If it should happen that those
  platforms are not available any more, the data can easily be moved
  away to another platform. Datasets which are no longer being
  actively worked on will also be archived in the University's
  research data repository and on zenodo.\label{Sagepres}
\end{description}


\begin{enumerate}





\item {Dataset 1}


\begin{description}
\item[Name of data] Addition to the \GAP codebase
\item[Nature of data] Software
\item[Licence] GPL
\item[Reuse of existing data] The data is added to the already large existing \GAP codebase.
\item[Means of production] Code implementation by UStan and other
  participants, building on existing elements of \GAP, which have been
  implemented by many people.
\item[Data standard] Follows the conventions of \GAP.
\item [Usage for further experiments] The code is merged in the software and can be distributed and reused through the Software. Through the git history,
one can trace back older versions of the code and re-enable a former state of the software.
\item [Link] \href{http://www.gap-system.org}{http://www.gap-system.org}
\end{description}

%
% Alexander, Markus, more?
%
\end{enumerate}

\subsection{CNRS}

The data created by the CNRS concerns mainly contributions to the softwares \PARIGP and \Sage.
\begin{description}
\item[Data storage and security] All additions to the \PARIGP software are stored on their server which is hosted at the University of Bordeaux, France.
\item[Dissemination] The \PARIGP code, documentation and various binaries are publicly available on the server \href{http://pari.math.u-bordeaux.fr/}{pari.math.u-bordeaux.fr}. These are also packaged for major linux distributions.
\item[Preservation and future access] The \PARIGP software has existed
  since 1979 and its website since 2003. It gets supports from various
  French and European institutions and has been hosted at the University of Bordeaux since its creation.
\end{description}


\subsection{Jacobs University}

The data created by the Jacobs University team will be in the form of OMDoc/MMT
flexiformalizations (representations of mathematical knowledge and data at flexible levels
of formality). Most data will be generated by transforming and semantic preloading of
existing data sources (the mathematical data bases from WP6.)

All data will be hosted publicly on the MathHub portal (\url{http://mathhub.info}), a
dedicated information portal for active documents and data (flexiformal knowledge with
integrated semantic services). MathHub data is stored, versioned, and protected by the
state-of-the art GIT system.

Original data and will be licensed under an open knowledge license (see
\url{http://opendefinition.org}), transformed data will be licensed as open as the
original license allows it.

\subsection{University of Zurich}
The data created by the University of Zurich team will be flexiformalizations (representations of mathematical knowledge and data at flexible levels of formality). 
This will serve as connecting glue for the process of semantic preloading mentioned by Jacobs. 
Since this data will aim to provide specifications for software, flexiformalisations for mathematical knowledge, and specifications for the implementation of mathematical knowledge into mathematical software, this process might require many different formats, all to be ingested into the MMT system developed by Jacobs University. 
It is the main topic of WP6 to design a sustainable, flexible and distributed system that will allow for such efforts. We can expect that all the data produced will be protected under GIT as well, and hosted on either \url{http://mathhub.info}, GitHub or similar services. 
In order to enable computer algebra software systems such as Sage and GAP to benefit from this system, some of the data might be packaged as part of those distributions (through snapshotting of the federation of GIT repositories hosting data).
Original data and will be licensed under an open knowledge license (see\url{http://opendefinition.org}.

\subsection{University of Sheffield}

The outputs of the University of Sheffield will be mainly in the form of code. Where other data is needed it will be as a prerequisite for running a particular demonstration. 
\begin{description}
\item[Data storage and security] The code will be made available by github, or other suitable software version control and management systems, under BSD licenses. The code is public so there are no concerns about unauthorised access. For backups we will be relying on the distributed nature of git storage and back up facilities managed by the repository.

\item[Dissemination] Code will be available for dissemination by public accessibility and through the Open Data Science website (http://opendsi.cc) which is also github hosted.

\item[Preservation and future access] The git model ensures we will have local back ups of repositories across multiple machines, but the main data provision moving forward will be github. The University of Sheffield also has deals with figshare for making data available. We will exploit this mechanism of sharing as appropriate.
\end{description}

\subsection{University of Southampton}

There are no significant data sets associated with the work at Southampton. The most important data is resulting code and associated documentation and tutorials. The details below refer to this data set, and we expect the data set to be fairly small (order of 1 GB).
\begin{description}
\item[Data storage and security] Data Storage: The code is stored in a distributed repository (git at the moment), and a central clone of this repository is stored with Github.com in the cloud. We may use multiple repositories, and store a central copy of each on Github.com.

  Security: All all the code is public, and there no concern about unauthorised access. Through cloud hosting and local clones of repositories, there are backups and redundancy.
\item[Dissemination] Data can be accessed through the public repositories, and the public website (probably this URL: \href{http://joommf.github.io}{http://joommf.github.io}, tbc), providing open access.
\item[Preservation and future access] We rely on provision of the data through \href{github.com}{github.com} but maintain local copies of the repository in case github.com ceases to exist or suffers from catastrophic technology failure. It is likely that other online repository hosting providers would be able to fill the gap (bitbucket.org is an existing alternative). The University of Southampton offers long term storage of small data sets for 10 years -- the repositories would fall into this categories. While the data wouldn't be conveniently accessible, this provides an extra layer of backups, from which accessible repositories and websites could be created easily.
\end{description}


\subsection{Simula Research Laboratory}

This subsection will contain all datasets Simula Research Laboratory
(SRL) is currently able to describe.

All data sets produced by the work at SRL so far is in the form of
source code and associated documentation and example files.
The size of the produced data is small, on the order of tens of MB.

\begin{description}
%  Quickly explain how data are stored and protected within yout institution
\item[Data storage and security] All source code is stored in public
  repositories on www.github.com using the distributed version control
  system git. This means full copies of all files including their edit
  history are located on both external cloud infrastructure with
  professional backup routines and the personal computers of
  developers, and in the event of failure of either system restoring
  is trivial.

% How data can be disseminated -> openaccess etc
\item[Dissemination] Data can be accessed through the public Github
  repositories, providing open access. All source code is published
  under the standard open source licence for source code related to
  the Jupyter project, namely the ``Modified BSD License''.

% How data can be preserved and available in the next years
\item[Preservation and future access] All data from SRL is published
  through public repositories under an open source licence, and
  copyright is assigned to the Jupyter project. This ensures the
  results can be kept alive and developed further alongside the
  Jupyter project. The Jupyter project has multiple international
  partners, both inside and outside Europe, both academic and
  commercial, ensuring continuation far beyond the end of
  OpenDreamKit.

\end{description}

\begin{enumerate}

  \item nbdime, a new subproject of Jupyter for diff and merge of Jupyter notebooks
    \begin{description}
    \item[Name of data] nbdime project
    \item[Nature of data] Software code
    \item[Licence] Modified BSD Licence
    \item[Reuse of existing data] The data reuses conventions from the
      existing Jupyter codebase, and reuses external open source
      software libraries where applicable.
    \item[Mean of production] Code implementation by SRL participants.
    \item[Data standard] The code is written in Python and Javascript, using Jupyter coding conventions.
    \item [Usage for further experiments] When completed, researchers
      and developers can use this tool to merge Jupyter notebooks
      when working with git repositories, an important improvement to
      a reproducible scientific workflow.
    \item [Link] \href{https://github.com/martinal/nbdime}(https://github.com/martinal/nbdime)
    \end{description}

\end{enumerate}

\subsection{University of Warwick}

The datasets produced by the University of Warwick which are relevant
to ODK are all related to with the LMFDB database, which is currently
hosted at Warwick.  As well as the LMFDB database itself, the
different components of the data are created and stored in a variety
of places, and we only list here those for which researchers at
Warwick are responsible.

\begin{enumerate}

\item{The LMFDB database}
\begin{description}
\item[Name of data] The LMFDB database
\item[Licence]  Under discussion by the LMFDB developers.
\item[Nature of data] A mongo database.  For a detailed description of
  its contents, see \url{https://github.com/LMFDB/lmfdb-inventory}.
  The website \url{http://www.lmfdb.org/} provides a user interface to
  the database and documents its contents, including its origin,
  extent and reliability.
\item[Reuse of existing data] Some of the data in LMFDB existed for
  many years, for example the Cremona Elliptic Curve Database (see
  {\tt ecdata} below), while others have been computed specifically
    for the project.   Contributors to LMFDB are listed at
    \url{http://www.lmfdb.org/acknowledgment}.
\item[Means of production] Computers, mostly using special purpose
  custom-written software, which in itself constitutes a significant
  research output by the contributors.
\item[Data standard] Each section of the LMFDB has its data quality
  documented and accessible via the website.  For example, see
  \url{http://www.lmfdb.org/EllipticCurve/Q/Source}.
\item [Usage for further experiments] All the LMFDB data is accessible
  through its website.  Future plans include provision of an API for
  accessing the data systematically.
\item [Link] \url{http://www.lmfdb.org/}
\end{description}

\item{{\tt ecdata}}
\begin{description}
\item[Name of data] The Cremona Elliptic Curve Database
\item[Licence]  Under discussion.
\item[Nature of data] A collection of plain text files containing
  tables of elliptic curves defined over~\Q, together with their
  arithmetic invariants, contained in a {\tt git} repository at
  \url{https://github.com/JohnCremona/ecdata}.  For a detailed
  technical description of their content and format, see
  \url{https://github.com/JohnCremona/ecdata/blob/master/doc/file-format.txt}.
  The website \url{http://johncremona.github.io/ecdata/} provides a
  simple user interface to the database and documents its contents,
  including its origin, extent and reliability.
\item[Reuse of existing data] All this data was computed, by John
  Cremona, with additional contributions from Andrew Sutherland and
  Jeremy Rouse.
\item[Means of production] Computers, mostly using special purpose
  custom-written software, which in itself constitutes a significant
  research output by the contributors.
\item[Data standard] Documented at
\url{http://johncremona.github.io/ecdata/}.
\item [Usage for further experiments] All the data in {\tt ecdata} is
  made available through the following channels: as an optional package
  in \Sage\ (with a small subset as standard); as an optional package
  in \PARIGP; as standard in \Magma; and through the LMFDB.  All of
  these, allow all researchers free access to use the data for their
  own investigations.
\item [Link] \url{http://johncremona.github.io/ecdata/}
\end{description}

\item{{\tt ecnf-data}}
\begin{description}
\item[Name of data] Database of Elliptic Curve over number fields
\item[Licence]  Under discussion.
\item[Nature of data] A collection of plain text files containing
  tables of elliptic curves defined over algebraic number fields other
  than~\Q, together with their arithmetic invariants, contained in a
  {\tt git} repository at \url{https://github.com/JohnCremona/ecnf-data}.
  For a detailed technical description of their content and format,
  see \url{https://github.com/JohnCremona/ecnf-data/blob/master/ecnf-format.txt}.
\item[Reuse of existing data] This data was computed, by John Cremona
  and several others.
\item[Means of production] Computers, mostly using special purpose
  custom-written software, which in itself constitutes a significant
  research output by the contributors.
\item[Data standard] Not yet documented.
\item [Usage for further experiments] All the data in {\tt ecnf-data}
  is made available through the LMFDB, allowing all researchers free
  access to use the data for their own investigations.
\item [Link] \url{http://johncremona.github.io/ecnf-data/}
\end{description}

\item{{\tt bianchi-data}}
\begin{description}
\item[Name of data] Database of Bianchi modular forms
\item[Licence]  Under discussion.
\item[Nature of data] A collection of plain text files containing
  tables of Bianchi modular newforms of degree~$1$ over imaginary
  quadratic fields of class number~$1$, contained in a
  {\tt git} repository at \url{https://github.com/JohnCremona/bianchi-data}.
\item[Reuse of existing data] This data was computed by John Cremona.
\item[Means of production] Computers, using special purpose
  custom-written software, which in itself constitutes a significant
  research output by the contributor.
\item[Data standard] Not yet documented.
\item [Usage for further experiments] All the data in {\tt
  bianchi-data} will be made available through the LMFDB, allowing all
  researchers free access to use the data for their own
  investigations.
\item [Link] \url{http://johncremona.github.io/bianchi-data/}
\end{description}



\end{enumerate}


\subsection{The University of Oxford}

The data created by Oxford mainly contributes to \Sage and \GAP.
\begin{description}
\item[Data storage and security] Data storage and security of \Sage and \GAP codebases and data libraries
 is addressed in items \ref{Sagesec} and \ref{GAPsec} above.`
\item[Dissemination]  Dissimination of \Sage and \GAP is addressed in items \ref{Sagediss} and \ref{GAPdiss} above.
\item[Preservation and future access] of \Sage and \GAP is addressed in items \ref{Sagepres} and \ref{GAPpres} above.
\end{description}

In particular, Oxford contributes to the following \Sage datasets.
\begin{enumerate}
\item[SRGs]
\begin{description}
\item[Name of data] Database of strongly regular graphs.
\item[Licence]  GPL.
\item[Nature of data] A mix of data generators implemented in Python and
data stored as json objects. A description can be found in \href{http://arxiv.org/abs/1601.00181}.
\item[Reuse of existing data] All this data was computed, by Nathann Cohen, Dmitrii Pasechnik,
and several other \Sage contributors.
\item[Means of production] Computers, mostly using \Sage and \GAP, and some purpose-written
Python code.
\item [Usage for further experiments] All the data  in this item is available from \Sage.
\item [Link] \url{http://doc.sagemath.org/html/en/reference/graphs/sage/graphs/strongly_regular_db.html}
\end{description}
\item[Hadamard matrices of special types]
\begin{description}
\item[Name of data] Database of special Hadamard matrices.
\item[Licence]  GPL.
\item[Nature of data] A mix of data generators implemented in Python and hardcoded as text data.
\item[Reuse of existing data] All this data was computed, by Nathann Cohen, Dmitrii Pasechnik,
and several other \Sage contributors.
\item[Means of production] Computers, mostly using \Sage and \GAP, and some purpose-written
Python code.
\item [Usage for further experiments] All the data  in this item is available from \Sage.
\item [Link] \url{http://doc.sagemath.org/html/en/reference/combinat/sage/combinat/matrices/hadamard_matrix.html}
\end{description}
\end{enumerate}

\subsection{UJF}

Most of the data created by UJF is related to software contributions
incorporated the codebase if the four software \texttt{LinBox}, \texttt{Givaro},
\texttt{FFLAS-FFPACK} and \Sage. 
There might also be smaller data sets of tutorials, documentation and teaching
content independent of the \Sage codebase which will be stored accordingly to
the size and needs.
Lastly, there will also be data related to experiments used in scientific
publications, mostly timing of experiments of high performance computation code,
together with the scripts and the exact description of the software stack used
to generate these timings.
In order to adhere to the standards of reproducible research, we plan to store the
sources of these publications, together with the data of the experiments and all
scripts used for their generation in a public distributed git repository and
store a central clone of it on platform such as github.

\begin{description}
\item[Data storage and security] All addition to the software codebases will be
  stored within the distributed repository of each software, either on \Sage's trac server
  \href{http://trac.sagemath.org/}{trac.sagemath.org} or the github central
  clone. For smaller datasets, including results of experiments, and publication
  sources, we will use other distributed git repositories and store a central clone on
  platforms such as github. All the present data is public, and there is no
  concern about unauthorised access. Through cloud hosting and local clones of
repositories, there are backups and redundancy.
\item[Dissemination] The codebase of all software contributed to is publicly  accessible through either the trac server
  \href{http://trac.sagemath.org/}{trac.sagemath.org} and distributed within the
  \Sage software or the git repositories of other software. For other data, we have an open access and open source policy and will advertise the data sets accordingly.
\item[Preservation and future access] By using the distributed system git to manage most of our data, we assure a local copy of the data within each participant machine. We rely on external platforms (trac and github) for public access. If it should happen that those platforms are not available any more, the data can easily be moved away to another platform.
\end{description}


\begin{enumerate}

\item {Contibution to \Sage}

\begin{description}
\item[Name of data] Addition to the \Sage codebase
\item[Nature of data] Software code
\item[Licence] GPL
\item[Reuse of existing data] The data is added to the already large existing \Sage codebase.
\item[Mean of production] Code implementation by UJF participants.
\item[Data standard] The code is mostly written in Python and Cython, also using the Rest syntax for documentation and \Sage coding conventions.
\item [Usage for further experiments] The code is merged in the software and can be distributed and reused through the Software. Through the git history,
one can trace back older versions of the code and re-enable a former state of the software.
\item [Link] \href{http://trac.sagemath.org/}{trac.sagemath.org}
\end{description}


\item{Contribution to \texttt{LinBox}, \texttt{Givaro}, \texttt{FFLAS-FFPACK}}
\begin{description}
\item[Name of data] Addition to the codebase of the \texttt{LinBox},
  \texttt{Givaro} and \texttt{FFLAS-FFPACK} software
\item[Nature of data] Software code
\item[Licence] LGPL
\item[Reuse of existing data] The data is added to the already large existing codebase.
\item[Mean of production] Code implementation by UJF participants.
\item[Data standard] The code is mostly written in C++, also using the Doxygen syntax for documentation.
\item [Usage for further experiments] The code is merged in the software and can be distributed and reused through the Software. Through the git history,
one can trace back older versions of the code and re-enable a former state of the software.
\item [Link] \href{https://github.com/linbox-team}{github.com/linbox-team}
\end{description}

\item{Data of experimental results published in scientific publications}

\begin{description}
\item[Name of data] Experimental results of parallel computation benchmarks,
\item[Nature of data] any data related to an experiment, including timings,
  memory usage, input data, scripts used to run the experiments and description
  of the software stack used for their production
\item[Licence] Creative commons BY-ND for experiments' data, GPL for scripts
\item[Reuse of existing data] The data is open for reuse without modification.
\item[Mean of production] Experiments run by UJF participants.
\item[Data standard] Not yet documented
\item [Usage for further experiments] All information provided should adhere to
  the standards of reproducible research, in order to allow reproduction of
  this data.
\item [Link] None yet
\end{description}


\end{enumerate}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
